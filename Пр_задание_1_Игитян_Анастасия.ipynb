{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNovbx/K9ZnS8Fgqwgh9t9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeikoFudo/IT-school/blob/main/%D0%9F%D1%80_%D0%B7%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_1_%D0%98%D0%B3%D0%B8%D1%82%D1%8F%D0%BD_%D0%90%D0%BD%D0%B0%D1%81%D1%82%D0%B0%D1%81%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfTNj762aIsy",
        "outputId": "7a8af0bc-6877-4d18-e982-2d38ee0dad57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.5.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting slovnet>=0.6.0\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.1-py3-none-any.whl (33 kB)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.22.4)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=bc56fb0464239bc556ba61950a2e019674b33e515386c823786cdc82cd8fe9c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26114 sha256=0250440c2a41f8393f0521f509cb16779c81ba7fcbc27c166a4757b0aa4f0ede\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.5.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попытка решения данной задачи с помощью выделения коллокаций \n"
      ],
      "metadata": {
        "id": "DmNT9ULWrhmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Импорт необходимых для задачи библиотек\n",
        "from natasha import MorphVocab, Doc, Segmenter \n",
        "from random import choice, randint\n",
        "from string import punctuation\n",
        "\n",
        "#Открываю файл с несколькими текстами на осетинском языке\n",
        "file = open('oss_text.txt', 'r')\n",
        "ose_text = file.read()\n",
        "file.close()\n",
        "\n",
        "myPunctuation = ['!', '.', '?', '...']\n",
        "\n",
        "#Токенизация \n",
        "morph_vocab = MorphVocab()\n",
        "segmenter = Segmenter(morph_vocab)\n",
        "doc = Doc(ose_text)\n",
        "doc.segment(segmenter)\n",
        "\n",
        "#Добавляю токены(слова) в список \n",
        "words = []\n",
        "for token in doc.tokens:\n",
        "    if token.text.isalpha():\n",
        "        words.append(token.text)\n",
        "\n",
        "#print(words)\n",
        "\n",
        "def generate_word(word):\n",
        "    cnt = words.count(word)\n",
        "    if cnt != 0:\n",
        "        ind = randint(1, cnt)\n",
        "        k = 0\n",
        "        for i, w in enumerate(words):\n",
        "            if w == word:\n",
        "                k += 1\n",
        "                if k == ind:\n",
        "                    if ind == len(words) - 1:\n",
        "                        return choice(words)\n",
        "                    else:\n",
        "                        return words[i + 1]\n",
        "    else:\n",
        "        return choice(words)\n",
        "\n",
        "\n",
        "def generate_sentence(begin_word):\n",
        "    n = randint(5, 10)\n",
        "    sen = begin_word\n",
        "    new_word = begin_word\n",
        "    for i in range(n):\n",
        "        new_word = generate_word(new_word)\n",
        "        sen += \" \" + new_word\n",
        "    return sen\n",
        "\n",
        "\n",
        "words = list(filter(lambda x: x not in punctuation, words))\n",
        "\n",
        "print(generate_sentence(\"Афтæ\")+choice(myPunctuation))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlbWx3ErfwYg",
        "outputId": "3c3cf828-76cb-4da4-fda9-1753f4cf3349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Афтæ цъус куы нæ фæччы хъазын æмæ Ахмæтимæ не вдисынц.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попытка решения данной задачи с помощью выделения коллокаций "
      ],
      "metadata": {
        "id": "KOKzmYsgrkOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Импорт необходимых для задачи библиотек\n",
        "from natasha import MorphVocab, Doc, Segmenter\n",
        "from random import choice, randint\n",
        "from string import punctuation\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "\n",
        "#Открываю файл с несколькими текстами на осетинском языке\n",
        "file = open('oss_text.txt', 'r')\n",
        "ose_text = file.read()\n",
        "file.close()\n",
        "\n",
        "#Токенизация \n",
        "morph_vocab = MorphVocab()\n",
        "segmenter = Segmenter(morph_vocab)\n",
        "doc = Doc(ose_text)\n",
        "doc.segment(segmenter)\n",
        "\n",
        "#Добавляю токены(слова) в список \n",
        "words = []\n",
        "for token in doc.tokens:\n",
        "    if token.text.isalpha():\n",
        "        words.append(token.text)\n",
        "\n",
        "#Cоздаю экземпляр класса, который предоставляет методы для подсчета статистических связей между биграммами \n",
        "bigram_measures = BigramAssocMeasures() \n",
        "#Нахожу коллокации-биграммы в тексте, представленном списком слов\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "#Фильтрую и оставляю только те, что встречаются не менее 5 раз \n",
        "finder.apply_freq_filter(5)\n",
        "#Добавляю в список 30 коллокаций с самым высоким PMI(Та самая штука из курса ВШЭ по анализу текстов)\n",
        "collocations = finder.nbest(bigram_measures.pmi, 30)\n",
        "#print(type(collocations))\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_sentence(words, collocations, n):\n",
        "\n",
        "    \"\"\"\n",
        "    На каждом шаге функция generate_sentence выбирает случайное слово из списка слов, \n",
        "    а затем выбирает следующее слово из списка коллокаций,\n",
        "    если предыдущее слово встречалось в первой части коллокации.\n",
        "    Если таких коллокаций нет, то выбирается новое случайное слово из списка слов.\n",
        "    \"\"\"\n",
        "    sentence = [random.choice(words)]\n",
        "    while len(sentence) < n: #Генерирую предложение длины n\n",
        "        last_word = sentence[-1]\n",
        "        for collocation in collocations:\n",
        "            if last_word == collocation[0]:\n",
        "                sentence.append(collocation[1])\n",
        "                break\n",
        "        else:\n",
        "            sentence.append(random.choice(words))\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "n = int(input('Сколько ещё бреда тебе надобно старче?'))\n",
        "print(generate_sentence(words, collocations, n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Ks6zY4rb4z",
        "outputId": "eeb3da60-7a69-41b4-f018-d04043285060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сколько ещё бреда тебе надобно старче?6\n",
            "хъуамæ сæ къуымы æхсыстой уыдон Принц\n"
          ]
        }
      ]
    }
  ]
}